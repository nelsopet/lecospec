{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import rasterio\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.mllib import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "input_filepath = \"../Data/SubsetDatacube\"\n",
    "input_data = rasterio.open(input_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.getPrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raster_to_df(raster, spark_df, spark_context):\n",
    "    # appends raster data to spark data frame\n",
    "    temp_df = spark_context.createDataFrame()\n",
    "    for x in range(raster.width):\n",
    "        # Store Aggregate all of the rows over Y; does 1 union per x value.  \n",
    "        # this is meant to be a compromise between memory and performance\n",
    "        # running a repartition after this operation is recommended\n",
    "        rows = []\n",
    "        for y in range(raster.height):\n",
    "            row = (x, y) + (raster.read(i)[x,y] for i in raster.indexes)\n",
    "            rows.append(row)    \n",
    "        # append rows as spark data frame\n",
    "    newrows = spark.createDataFrame([pixel_values], spark_df.columns)\n",
    "        # this is bad functional programming, but I am doing it to save memory\n",
    "     return spark_df.union(newrows)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pixel_deriv(pixel_vals):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}